{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "300aeda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from scipy.ndimage import gaussian_filter, maximum_filter, generate_binary_structure, iterate_structure\n",
    "from itertools import chain\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c76e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directory and save folder\n",
    "os.chdir('/Users/hadrienpadilla/Documents/McGill/Peyrache Lab/pRNN')\n",
    "savefolder = 'Data/hadrien_analyzed_nets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2afe50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _to_2d(a):\n",
    "    \"\"\"Return a 2-D float array from array-like or dict->{array}.\"\"\"\n",
    "    if a is None or (isinstance(a, float) and np.isnan(a)):\n",
    "        raise ValueError(\"missing\")\n",
    "    if isinstance(a, dict):\n",
    "        a = a[0] if 0 in a else a[min(a.keys())]\n",
    "    a = np.asarray(a, dtype=float).squeeze()\n",
    "    if a.ndim == 2:\n",
    "        return a\n",
    "    if a.ndim == 3:\n",
    "        return a[..., 0] if a.shape[-1] <= 4 else a[0, ...]\n",
    "    raise ValueError(f\"expected 2D array, got {a.shape}\")\n",
    "\n",
    "#normalize values between 0 and 1 \n",
    "def _normalize(a):\n",
    "    a = np.asarray(a, dtype=float)\n",
    "    lo, hi = np.nanmin(a), np.nanmax(a)\n",
    "    if not np.isfinite(lo) or not np.isfinite(hi) or hi <= lo:\n",
    "        return np.zeros_like(a)\n",
    "    return (a - lo) / (hi - lo)\n",
    "\n",
    "# ---------- peak finding ----------\n",
    "def _peak_coords(a2d, threshold=0.3, neighborhood=2, min_distance=3, sigma=0.8):\n",
    "    # Get raw and smoothed arrays\n",
    "    raw_arr = np.array(a2d)\n",
    "    smooth_arr = gaussian_filter(a2d, sigma=sigma)\n",
    "    \n",
    "    # Create footprint for local maximum detection\n",
    "    fp = generate_binary_structure(raw_arr.ndim, neighborhood)\n",
    "    \n",
    "    # Find peaks separately in raw and smoothed data\n",
    "    raw_peaks = np.argwhere((raw_arr == maximum_filter(raw_arr, footprint=fp)) & \n",
    "                           (raw_arr >= threshold))\n",
    "    smooth_peaks = np.argwhere((smooth_arr == maximum_filter(smooth_arr, footprint=fp)) & \n",
    "                              (raw_arr >= threshold))  # Still use raw_arr for threshold\n",
    "    \n",
    "    # Combine peaks\n",
    "    all_peaks = np.unique(np.vstack([raw_peaks, smooth_peaks]), axis=0)\n",
    "    \n",
    "    # Sort peaks by raw value (highest first)\n",
    "    peak_values = raw_arr[all_peaks[:, 0], all_peaks[:, 1]]\n",
    "    sorted_idx = np.argsort(-peak_values)\n",
    "    all_peaks = all_peaks[sorted_idx]\n",
    "    \n",
    "    # Filter by minimum distance\n",
    "    filtered_peaks = []\n",
    "    for peak in all_peaks:\n",
    "        if not filtered_peaks:\n",
    "            filtered_peaks.append(peak)\n",
    "            continue\n",
    "        \n",
    "        distances = np.sqrt(np.sum((np.array(filtered_peaks) - peak)**2, axis=1))\n",
    "        if np.all(distances > min_distance):\n",
    "            filtered_peaks.append(peak)\n",
    "    \n",
    "    return [tuple(p) for p in filtered_peaks]\n",
    "    \n",
    "\n",
    "\n",
    "def identify_peaks(cells, *, threshold=0.5, neighborhood=1, normalize=True, cell_ids=None, sigma = 1.0, min_distance = 5):\n",
    "    out = {}\n",
    "    for i, cell in enumerate(cells):\n",
    "        try:\n",
    "            a = _to_2d(cell)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if normalize:\n",
    "            a = _normalize(a)\n",
    "        key = cell_ids[i] if (cell_ids is not None and i < len(cell_ids)) else i\n",
    "        out[key] = _peak_coords(a, threshold=threshold, neighborhood=neighborhood, min_distance = min_distance, sigma = sigma)\n",
    "    return out\n",
    "\n",
    "def _grid_xy(env, H, W):\n",
    "    \"\"\"Return (H,W,2) array of bin centers (x,y).\"\"\"\n",
    "    e = getattr(env, \"env\", env)\n",
    "    coords = getattr(e, \"discrete_coords\", None)\n",
    "    if coords is not None and np.asarray(coords).shape[:2] == (H, W):\n",
    "        return np.asarray(coords)\n",
    "    xedges, yedges = getattr(e, \"xedges\", None), getattr(e, \"yedges\", None)\n",
    "    if xedges is not None and yedges is not None:\n",
    "        xc = (np.asarray(xedges)[:-1] + np.asarray(xedges)[1:]) / 2\n",
    "        yc = (np.asarray(yedges)[:-1] + np.asarray(yedges)[1:]) / 2\n",
    "        X, Y = np.meshgrid(xc, yc)\n",
    "        return np.stack([X, Y], -1)\n",
    "    X, Y = np.meshgrid((np.arange(W) + 0.5) / W, (np.arange(H) + 0.5) / H)\n",
    "    return np.stack([X, Y], -1)\n",
    "\n",
    "\n",
    "def peaks_to_xy_centers(peaks_by_cell, pf_dict, env):\n",
    "    # discover grid\n",
    "    first = next(pf_dict[k] for k in peaks_by_cell if k in pf_dict)\n",
    "    H, W = _to_2d(first).shape\n",
    "    grid = _grid_xy(env, H, W)\n",
    "\n",
    "    out = {}\n",
    "    for cid, rc_list in peaks_by_cell.items():\n",
    "        if cid not in pf_dict or not rc_list:\n",
    "            out[cid] = []\n",
    "            continue\n",
    "        a2d = _to_2d(pf_dict[cid])\n",
    "        xy = []\n",
    "        for r, c in rc_list:\n",
    "            ri, ci = int(r), int(c)\n",
    "            x, y = grid[ri, ci]\n",
    "            xy.append((float(x), float(y)))\n",
    "        out[cid] = xy\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d092e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_analysis = pd.read_pickle('Data/hadrien_analyzed_nets/base/base_net.pkl')\n",
    "base_net = pd.read_pickle('nets/for_Hadrien/base/multRNN_5win_i2_o2-no_reward-s1042_ep5-cpu.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329f1449",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_analysis = pd.read_pickle('Data/hadrien_analyzed_nets/base/base_net.pkl')\n",
    "curr_analysis = base_analysis\n",
    "base_net = pd.read_pickle('nets/for_Hadrien/base/multRNN_5win_i2_o2-no_reward-s1042_ep5-cpu.pkl')\n",
    "\n",
    "bins = curr_analysis.bins\n",
    "circles = curr_analysis.circles\n",
    "\n",
    "\n",
    "# explode -> list of dicts\n",
    "cells_s = base_analysis.cells.explode()\n",
    "cells_l = cells_s.tolist()\n",
    "# positions in the flattened list (0..K-1) where type == 'complex'\n",
    "complex_pos = [i for i, d in enumerate(cells_l)\n",
    "               if isinstance(d, dict) and d.get('type') == 'complex']\n",
    "# the external IDs (your 'idx' values) for later labeling\n",
    "complex_ids = [d['idx'] for d in cells_l\n",
    "               if isinstance(d, dict) and d.get('type') == 'complex']\n",
    "pf_dict = base_net.TrainingSaver.place_fields[0]\n",
    "env_obj = base_net.EnvLibrary[0]\n",
    "ids = [cid for cid in complex_ids if cid in pf_dict]\n",
    "complex_pfs = [pf_dict[cid] for cid in ids]\n",
    "peaks = identify_peaks(complex_pfs, threshold=0.6, cell_ids = ids)\n",
    "xy_by_peak = peaks_to_xy_centers(peaks, pf_dict, env_obj)\n",
    "\n",
    "bins['single+complex_mult_peaks'] = build_bins\n",
    "\n",
    "\n",
    "base_net.circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc5f7957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    {'single': {'counts': [12, 18, 19, 24, 32, 40]...\n",
      "Name: bins, dtype: object\n",
      "{'single': {'counts': [12, 18, 19, 24, 32, 40], 'fractions': [0.08275862068965517, 0.12413793103448276, 0.1310344827586207, 0.16551724137931034, 0.2206896551724138, 0.27586206896551724]}, 'single+complex': {'counts': [38, 47, 49, 92, 124, 99], 'fractions': [0.08463251670378619, 0.10467706013363029, 0.1091314031180401, 0.20489977728285078, 0.27616926503340755, 0.22048997772828507]}, 'single+complex_mult_peaks': {'counts': [1109, 567, 19, 24, 32, 82], 'fractions': [0.6050190943807965, 0.309328968903437, 0.010365521003818877, 0.01309328968903437, 0.01745771958537916, 0.0447354064375341]}}\n",
      "{'single': {'counts': [1, 0, 0], 'fractions': [0.006896551724137931, 0.0, 0.0], 'radius': 0.05}, 'single+complex': {'counts': [2, 4, 0], 'fractions': [0.004454342984409799, 0.008908685968819599, 0.0], 'radius': 0.05}, 'single+complex_mult_peaks': {'counts': [1, 0, 0], 'fractions': [0.0005455537370430987, 0.0, 0.0], 'radius': 0.05}}\n"
     ]
    }
   ],
   "source": [
    "base_analysis = pd.read_pickle('Data/hadrien_analyzed_nets/base/base_net.pkl')\n",
    "bins = base_analysis.bins\n",
    "print(bins)\n",
    "print(bins[0])\n",
    "circles = base_analysis.circles\n",
    "print(circles[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81399b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".newenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
